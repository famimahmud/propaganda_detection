{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU8lES-HiQHE"
   },
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_z85btlfVSY"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKqXPIS-jWOf",
    "outputId": "83c98fc1-2c1c-4076-fdba-1edc73a9a54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nlp-lab-ws23/nlp_praktikum/persuasion_technique_detection/subtask2b\n",
      "Tue Jan 30 13:48:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 Ti     On  | 00000000:2B:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8              25W / 350W |      3MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# For emptying trash after each run\n",
    "\"\"\"from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from googleapiclient.discovery import build\n",
    "drive_service = build('drive', 'v3')\n",
    "drive_service.files().emptyTrash().execute()\"\"\"\n",
    "!pwd\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fEoAZr2jdYV"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "deheGLB8jglV"
   },
   "outputs": [],
   "source": [
    "folder_name = \"/home/nlp-lab-ws23/nlp_praktikum/persuasion_technique_detection/\" #\"/content/drive/MyDrive/persuasion_technique_detection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WhEmv5JrjkZA"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets wandb evaluate accelerate -qU sklearn_hierarchical_classification sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_oAACjMfjnfd"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import subprocess\n",
    "import json\n",
    "import warnings\n",
    "import shutil\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, ViTFeatureExtractor\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset,load_dataset,DatasetDict,concatenate_datasets\n",
    "import datasets\n",
    "import os\n",
    "from torch.nn.functional import sigmoid\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import Trainer\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoImageProcessor,AutoTokenizer,AutoFeatureExtractor,ViTImageProcessor,ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel,CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Esf7JoifjrV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "AVAIL_GPUS = 0\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    AVAIL_GPUS = torch.cuda.device_count()\n",
    "    print(f'There are {AVAIL_GPUS} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "namJ79hAjvvQ"
   },
   "source": [
    "## Login WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qEAikZgDjtj5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahmudfami\u001b[0m (\u001b[33mtumnlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "#wandb.login(relogin=True)\n",
    "wandb.login()\n",
    "\n",
    "# setup wandb environment variables\n",
    "os.environ['WANDB_PROJECT'] = \"subtask2b\"\n",
    "os.environ['WANDB_ENTITY'] = \"tumnlp\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]= \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kd_G2X1-j1uB"
   },
   "outputs": [],
   "source": [
    "text_checkpoint = \"vinai/bertweet-large\"\n",
    "img_checkpoint= \"google/vit-base-patch32-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N-2F4_ljj6bu"
   },
   "outputs": [],
   "source": [
    "summary_dir_path = folder_name + \"subtask2b/summaries_avg/summary_\" + text_checkpoint.replace(\"/\",\"_\")+\"_\"+img_checkpoint.replace(\"/\",\"_\")+\"FINAL_PREDICTIONS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdDPD3uXkMWY"
   },
   "source": [
    "## Preprocess text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "zXwsWCmbkQeT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 150 600\n"
     ]
    }
   ],
   "source": [
    "val_path=\"data/subtask2b/val.json\"\n",
    "train_path=\"data/subtask2b/train.json\"\n",
    "dev_path=\"data/subtask2b/dev_subtask2b_en.json\"\n",
    "test_path=\"data/subtask2b/en_subtask2b_test_unlabeled.json\"\n",
    "\n",
    "train_image_path=\"data/subtask2b/subtask2b_images/train/\"\n",
    "val_image_path=\"data/subtask2b/subtask2b_images/val/\"\n",
    "dev_image_path=\"data/subtask2b/subtask2b_images/dev/\"\n",
    "test_image_path=\"data/subtask2b/english/\"\n",
    "\n",
    "with open(folder_name+val_path) as f:\n",
    "  d = json.load(f)\n",
    "  val=pd.DataFrame.from_dict(d)\n",
    "  labels=val[\"label\"]\n",
    "  num_label=[int(el==\"propagandistic\") for el in labels]\n",
    "  val[\"num_label\"]=num_label\n",
    "  val=val.drop(columns=['label'])\n",
    "  val_set=val.rename(columns={\"num_label\": \"label\"})\n",
    "  val_set[\"origin\"]=[\"val\"] * len(val_set)\n",
    "\n",
    "with open(folder_name+train_path) as f:\n",
    "  d = json.load(f)\n",
    "  train=pd.DataFrame.from_dict(d)\n",
    "  labels=train[\"label\"]\n",
    "  num_label=[int(el==\"propagandistic\") for el in labels]\n",
    "  train[\"num_label\"]=num_label\n",
    "  train=train.drop(columns=['label'])\n",
    "  train_set=train.rename(columns={\"num_label\": \"label\"})\n",
    "  train_set[\"origin\"]=[\"train\"] * len(train_set)\n",
    "\n",
    "with open(folder_name+dev_path) as f:\n",
    "  d = json.load(f)\n",
    "  dev=pd.DataFrame.from_dict(d)\n",
    "  labels=dev[\"label\"]\n",
    "  num_label=[int(el==\"propagandistic\") for el in labels]\n",
    "  dev[\"num_label\"]=num_label\n",
    "  dev=dev.drop(columns=['label'])\n",
    "  dev_set=dev.rename(columns={\"num_label\": \"label\"})\n",
    "  dev_set[\"origin\"]=[\"dev\"] * len(dev_set)\n",
    "\n",
    "with open(folder_name+test_path) as f:\n",
    "  d = json.load(f)\n",
    "  test_set=pd.DataFrame.from_dict(d)\n",
    "\n",
    "label2num={\"non_propagandistic\":0,\"propagandistic\":1}\n",
    "num2label={0:\"non_propagandistic\",1:\"propagandistic\"}\n",
    "\n",
    "train_set = pd.concat([train_set, dev_set], ignore_index=True)\n",
    "print(len(train_set),len(val_set),len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "V7sKojuVkswn"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, dataset_type,image_processor):\n",
    "        super().__init__()\n",
    "        self.ids=list(dataset[\"id\"])\n",
    "        self.texts = list(dataset[\"text\"])\n",
    "        self.image_paths = list(dataset[\"image\"])\n",
    "        if dataset_type==\"train\" or dataset_type==\"val\":\n",
    "          self.labels = dataset[\"label\"].astype(int).tolist()\n",
    "          self.origin = list(dataset[\"origin\"])\n",
    "        self.image_processor = image_processor\n",
    "        self.dataset_type=dataset_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset_type==\"test\":\n",
    "          image_path=folder_name+ test_image_path+ self.image_paths[idx]\n",
    "        elif self.dataset_type==\"val\":\n",
    "          image_path=folder_name+ val_image_path+ self.image_paths[idx]\n",
    "        else: # train set\n",
    "          if self.origin[idx] ==\"train\": # original train set\n",
    "            image_path=folder_name+ train_image_path+ self.image_paths[idx]\n",
    "          else: # dev set\n",
    "            image_path=folder_name+ dev_image_path+ self.image_paths[idx]\n",
    "\n",
    "        image_input = self.image_processor(images=Image.open(image_path).convert(\"RGB\"), return_tensors=\"pt\")\n",
    "\n",
    "        if self.dataset_type==\"train\" or self.dataset_type==\"val\":\n",
    "          label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "          return self.ids[idx],self.texts[idx],image_input,label\n",
    "        else:\n",
    "          return self.ids[idx],self.texts[idx],image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8Unw0tBzsyGU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(text_checkpoint)\n",
    "text_model = AutoModel.from_pretrained(text_checkpoint)\n",
    "\n",
    "# change image processor for different models\n",
    "image_processor = ViTImageProcessor.from_pretrained(img_checkpoint)  #'google/vit\n",
    "image_model = AutoModel.from_pretrained(img_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-y3I4Vv1o2PA"
   },
   "outputs": [],
   "source": [
    "#image_model.config.hidden_dim,text_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QP4AyUorl6jn"
   },
   "outputs": [],
   "source": [
    "class TextImageBinaryClassifier(nn.Module):\n",
    "    def __init__(self, text_model, image_model):\n",
    "        super(TextImageBinaryClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "        image_hidden_size=image_model.config.hidden_size # google/vit\n",
    "\n",
    "        self.lin1 = nn.Linear(text_model.config.hidden_size + image_hidden_size,512)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.lin2=nn.Linear(512,1)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # Text encoding\n",
    "        text_outputs = self.text_model(**text_input)\n",
    "\n",
    "        text_embedding = text_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Image encoding\n",
    "        try:\n",
    "\n",
    "          image_outputs = self.image_model(**image_input)  #'google/vit\n",
    "          image_embedding = image_outputs.last_hidden_state.mean(dim=1) #'google/vit\n",
    "\n",
    "        except Exception as e:\n",
    "          print(\"error:\",str(e))\n",
    "\n",
    "\n",
    "        # Concatenate text and image embeddings\n",
    "        combined_embedding = torch.cat((text_embedding, image_embedding), dim=1)\n",
    "\n",
    "\n",
    "        x = self.lin1(combined_embedding)\n",
    "        x=self.relu(x)\n",
    "        logits=self.lin2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "dO1VOZ8XqiiL"
   },
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "train_dataset = CustomDataset(train_set,\"train\", image_processor)\n",
    "test_dataset = CustomDataset(test_set,\"test\", image_processor)\n",
    "val_dataset = CustomDataset(val_set,\"val\", image_processor)\n",
    "\n",
    "batch_size=2\n",
    "num_workers=2\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=num_workers,pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers=num_workers,pin_memory=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers,pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aLh0NGcmazEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1500 val size: 150 test size: 600\n"
     ]
    }
   ],
   "source": [
    "print(\"train size:\",len(train_set),\"val size:\",len(val_set),\"test size:\",len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIYgbiz9IBGr"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "difE4cnFJyls"
   },
   "outputs": [],
   "source": [
    "transfer_learning = False\n",
    "model_nodes = {\n",
    "    \"persuasion_or_not\":None}\n",
    "parent_nodes = {\n",
    "    \"persuasion_or_not\":\"persuasion_or_not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CKi7EtVyryYr"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  try:\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init()\n",
    "\n",
    "    # sweep agent inputs config with hyperparameters\n",
    "    config = wandb.config\n",
    "\n",
    "    learning_rate = config.learning_rate\n",
    "    run_name = config.run_name+f\"_{str(learning_rate)}learningRate\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    text_model = AutoModel.from_pretrained(text_checkpoint)\n",
    "\n",
    "    image_model = AutoModel.from_pretrained(img_checkpoint)\n",
    "\n",
    "    # Instantiate the custom model\n",
    "    model = TextImageBinaryClassifier(text_model, image_model)\n",
    "    model.cuda()\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_val_f1_macro=0.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        print(\"Started training epoch:\",epoch+1)\n",
    "        for id, text_input, image_input, label in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            text_input = tokenizer(text_input, return_tensors=\"pt\",truncation=True, padding=True,max_length=512)\n",
    "\n",
    "            text_input=text_input.to(\"cuda\")\n",
    "            image_input['pixel_values']=image_input['pixel_values'].squeeze(1)\n",
    "            image_input=image_input.to(\"cuda\")\n",
    "            label=label.to(\"cuda\")\n",
    "            logits = model(text_input, image_input)\n",
    "            preds=sigmoid(logits)\n",
    "            loss = loss_func(logits.squeeze(), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for metrics calculation\n",
    "            preds_classes = (preds > 0.5).int().cpu().numpy()\n",
    "            all_preds.extend(preds_classes)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Calculate and log F1 score and accuracy using sklearn metrics\n",
    "        f1_macro = f1_score(all_labels, all_preds,average=\"macro\")\n",
    "        f1_micro = f1_score(all_labels, all_preds,average=\"micro\")\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": average_loss, \"f1_macro\": f1_macro, \"f1_micro\":f1_micro, \"train_accuracy\": accuracy})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for id, text_input, image_input, label in val_dataloader:\n",
    "                text_input = tokenizer(text_input, return_tensors=\"pt\",truncation=True, padding=True,max_length=512)\n",
    "\n",
    "                text_input=text_input.to(\"cuda\")\n",
    "                image_input['pixel_values']=image_input['pixel_values'].squeeze(1)\n",
    "                image_input=image_input.to(\"cuda\")\n",
    "                label=label.to(\"cuda\")\n",
    "                logits = model(text_input, image_input)\n",
    "                preds=sigmoid(logits)\n",
    "                val_loss += loss_func(logits.squeeze(), label).item()\n",
    "                # Collect predictions and labels for metrics calculation\n",
    "                val_preds_classes = (preds > 0.5).int().cpu().numpy()\n",
    "                all_val_preds.extend(val_preds_classes)\n",
    "                all_val_labels.extend(label.cpu().numpy())\n",
    "        average_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # Calculate and log F1 score and accuracy for validation\n",
    "        val_f1_macro = f1_score(all_val_labels, all_val_preds,average=\"macro\")\n",
    "        val_f1_micro = f1_score(all_val_labels, all_val_preds,average=\"micro\")\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\"epoch\": epoch + 1, \"val_loss\": average_val_loss, \"val_f1_macro\": val_f1_macro,\"val_f1_micro\":val_f1_micro, \"val_accuracy\": val_accuracy})\n",
    "\n",
    "        # Save the model if the current validation f1_macro is better than the previous best\n",
    "        if val_f1_macro > best_val_f1_macro:\n",
    "          best_val_f1_macro = val_f1_macro\n",
    "          best_model_state_dict = model.state_dict()\n",
    "          wandb.log({\"eval/f1_macro\":val_f1_macro})\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training f1-macro: {f1_macro}, Validation f1-macro: {val_f1_macro}\")\n",
    "        model.train()\n",
    "    # Finish W&B run\n",
    "\n",
    "    artifact = wandb.Artifact(f\"best_model_{run_name}\".replace(\"/\",\"_\"), type=\"model\")\n",
    "    artifact.add_file(folder_name+f\"best_model_{run_name}.pth\".replace(\"/\",\"_\"), torch.save(best_model_state_dict, folder_name+f\"best_model_{run_name}.pth\".replace(\"/\",\"_\")))\n",
    "    wandb.run.log_artifact(artifact)\n",
    "    os.remove(folder_name+f\"best_model_{run_name}.pth\".replace(\"/\",\"_\"))\n",
    "    #drive_service.files().emptyTrash().execute()\n",
    "    wandb.finish()\n",
    "  except Exception as e:\n",
    "    print(f\"Error in training: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASo5_CiCHukV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wbcjdbwe\n",
      "Sweep URL: https://wandb.ai/tumnlp/subtask2b/sweeps/wbcjdbwe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xs2e7cyx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trun_name: vinai_bertweet-large-google_vit-base-patch32-224-in21k-subtask2b-FINAL_PREDICTIONS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nlp-lab-ws23/nlp_praktikum/persuasion_technique_detection/subtask2b/wandb/run-20240130_140041-xs2e7cyx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tumnlp/subtask2b/runs/xs2e7cyx' target=\"_blank\">likely-sweep-1</a></strong> to <a href='https://wandb.ai/tumnlp/subtask2b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tumnlp/subtask2b/sweeps/wbcjdbwe' target=\"_blank\">https://wandb.ai/tumnlp/subtask2b/sweeps/wbcjdbwe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tumnlp/subtask2b' target=\"_blank\">https://wandb.ai/tumnlp/subtask2b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tumnlp/subtask2b/sweeps/wbcjdbwe' target=\"_blank\">https://wandb.ai/tumnlp/subtask2b/sweeps/wbcjdbwe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tumnlp/subtask2b/runs/xs2e7cyx' target=\"_blank\">https://wandb.ai/tumnlp/subtask2b/runs/xs2e7cyx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training epoch: 1\n",
      "Epoch 1/10, Training f1-macro: 0.5064859891167532, Validation f1-macro: 0.6386916835699796\n",
      "Started training epoch: 2\n",
      "Epoch 2/10, Training f1-macro: 0.6555618294748728, Validation f1-macro: 0.6459627329192547\n",
      "Started training epoch: 3\n",
      "Epoch 3/10, Training f1-macro: 0.6838309124074955, Validation f1-macro: 0.6527777777777778\n",
      "Started training epoch: 4\n",
      "Epoch 4/10, Training f1-macro: 0.7467548856276387, Validation f1-macro: 0.730423620025674\n",
      "Started training epoch: 5\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparams in sweep configurations\n",
    "run_name=f'{text_checkpoint}-{img_checkpoint}-subtask2b-FINAL_PREDICTIONS'.replace(\"/\",\"_\")\n",
    "sweep_name=f'sweep_{run_name}'\n",
    "sweep_config = {\n",
    "    'method': 'grid',  # can be grid, random, or bayes\n",
    "    'name' : sweep_name,\n",
    "    'metric': {\n",
    "      'name': 'eval/f1_macro',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate' : {\n",
    "            'values': [1e-6,3e-6,5e-6,5e-5,5e-4]\n",
    "        },\n",
    "        'run_name': {\n",
    "            'value' : run_name\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start sweeps with specific configuration\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"subtask2b\")\n",
    "wandb.agent(sweep_id, train)\n",
    "# Get best model of sweep\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"subtask2b/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "artifacts = best_run.logged_artifacts()\n",
    "\n",
    "model_artifact = None\n",
    "for artifact in artifacts:\n",
    "    if 'model' in artifact.type:  # Adjust the condition based on your setup\n",
    "        model_artifact = artifact\n",
    "        break\n",
    "\n",
    "if model_artifact != None:\n",
    "  model_artifact_name = model_artifact.name\n",
    "  print(f\"Best Model: {model_artifact_name}\")\n",
    "else:\n",
    "  warnings.warn(f\"No models was found\")\n",
    "\n",
    "# save best model of this node\n",
    "model_nodes[\"persuasion_or_not\"] = model_artifact_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "id": "7W6NVBJvuMUv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-40 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 299, in _run_job\n",
      "    wandb.finish()\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 4110, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 420, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 361, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1953, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1966, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 462, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 303, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 4110, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 420, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 361, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1953, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1966, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/nlp-lab-ws23/nlp_praktikum/nlp_praktikum_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 462, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in training: DataLoader worker (pid(s) 2379808, 2379809) exited unexpectedly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-3</strong> at: <a href='https://wandb.ai/tumnlp/subtask2b/runs/m5zkr5hd' target=\"_blank\">https://wandb.ai/tumnlp/subtask2b/runs/m5zkr5hd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240130_135937-m5zkr5hd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g74zLLyO7Bl"
   },
   "source": [
    "## Eval on val set and save run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-TYPp7lO8NT"
   },
   "outputs": [],
   "source": [
    "def write_json(path,data,file_name=\"summary.json\"):\n",
    "  if not isinstance(data, dict):\n",
    "    data = data.to_dict(\"records\")\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "  with open(path+file_name, \"w\") as output_file:\n",
    "      json.dump(data, output_file, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzHD9xsmO9qH"
   },
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"text_checkpoint\" : text_checkpoint,\n",
    "    \"img_checkpoint\":img_checkpoint,\n",
    "    \"model_nodes\" : model_nodes,\n",
    "    \"train_path\" : train_image_path,\n",
    "    \"val_path\":val_image_path,\n",
    "    \"test_path\":test_image_path\n",
    "}\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOpLTcyLa7J3"
   },
   "outputs": [],
   "source": [
    "write_json(summary_dir_path,summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8OUFdEpObOa"
   },
   "source": [
    "### Eval on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xej5VYX6MRh3"
   },
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "artifact=api.artifact(model_nodes[\"persuasion_or_not\"])\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, model_nodes[\"persuasion_or_not\"].split(\":\")[0]+\".pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "\n",
    "\n",
    "text_model = AutoModel.from_pretrained(text_checkpoint)\n",
    "image_model = AutoModel.from_pretrained(img_checkpoint)\n",
    "# Instantiate the custom model\n",
    "model = TextImageBinaryClassifier(text_model, image_model)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVtL52LUMGOH"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "val_loss = 0.0\n",
    "all_val_preds = []\n",
    "all_val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for id, text_input, image_input, label in val_dataloader:\n",
    "        text_input = tokenizer(text_input, return_tensors=\"pt\",truncation=True, padding=True,max_length=512)\n",
    "        text_input=text_input.to(\"cuda\")\n",
    "        image_input['pixel_values']=image_input['pixel_values'].squeeze(1)\n",
    "        image_input=image_input.to(\"cuda\")\n",
    "        label=label.to(\"cuda\")\n",
    "        logits = model(text_input, image_input)\n",
    "        preds=sigmoid(logits)\n",
    "        val_loss += loss_func(logits.squeeze(), label).item()\n",
    "\n",
    "        # Collect predictions and labels for metrics calculation\n",
    "        val_preds_classes = (preds > 0.5).int().cpu().numpy()\n",
    "        all_val_preds.extend(val_preds_classes)\n",
    "        all_val_labels.extend(label.cpu().numpy())\n",
    "\n",
    "average_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "# Calculate and log F1 score and accuracy for validation\n",
    "val_f1_macro = f1_score(all_val_labels, all_val_preds,average=\"macro\")\n",
    "val_f1_micro = f1_score(all_val_labels, all_val_preds,average=\"micro\")\n",
    "val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "val_results={\"f1_macro\":val_f1_macro,\"f1_micro\":val_f1_micro,\"val_accuracy\":val_accuracy}\n",
    "print(val_results)\n",
    "summary[\"val_results\"]=val_results\n",
    "print(summary)\n",
    "\n",
    "val_pred_labels=[num2label[el.item()] for el in all_val_preds]\n",
    "val_dataset_=val_set.drop(columns=[\"label\",\"text\",\"image\"])\n",
    "val_dataset_[\"label\"]=val_pred_labels\n",
    "write_json(summary_dir_path,val_dataset_,\"val_preds.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmpeNtUXPFyU"
   },
   "outputs": [],
   "source": [
    "write_json(summary_dir_path, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE8G5_82PG19"
   },
   "outputs": [],
   "source": [
    "def delete_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"The directory {path} and all its contents have been deleted successfully\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error: {e.strerror}\")\n",
    "    else:\n",
    "        print(f\"The directory {path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WvHf-dhPNH6"
   },
   "outputs": [],
   "source": [
    "def delete_output_dirs(parent_directory):\n",
    "    for entry in os.listdir(parent_directory):\n",
    "        entry_path = os.path.join(parent_directory, entry)\n",
    "\n",
    "        if os.path.isdir(entry_path) and entry.startswith('output_'):\n",
    "            delete_dir(entry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsdOD9fIPP4H"
   },
   "outputs": [],
   "source": [
    "\"\"\"path = folder_name + \"subtask2b\"\n",
    "delete_output_dirs(path)\n",
    "delete_dir(path + \"tumnlp\")\n",
    "delete_dir(path + \"wandb\")\n",
    "delete_dir(path + \"artifacts\")\n",
    "delete_dir(path + \"tmp_trainer\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvy5O3nzKpR7"
   },
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "tTg5HTjZOv8B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best_model_vinai_bertweet-large-google_vit-base-patch32-224-in21k-subtask2b-FINAL_PREDICTIONS_5e-06learningRate:v0, 1695.30MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.1\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextImageBinaryClassifier(\n",
       "  (text_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (image_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=1792, out_features=512, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "artifact=api.artifact(model_nodes[\"persuasion_or_not\"])\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, model_nodes[\"persuasion_or_not\"].split(\":\")[0]+\".pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "\n",
    "\n",
    "text_model = AutoModel.from_pretrained(text_checkpoint)\n",
    "image_model = AutoModel.from_pretrained(img_checkpoint)\n",
    "# Instantiate the custom model\n",
    "model = TextImageBinaryClassifier(text_model, image_model)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "zSOrYP8xO7Oi"
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for id, text_input, image_input in test_dataloader:\n",
    "    text_input = tokenizer(text_input, return_tensors=\"pt\",truncation=True, padding=True,max_length=512)\n",
    "    text_input=text_input.to(\"cuda\")\n",
    "    image_input['pixel_values']=image_input['pixel_values'].squeeze(1)\n",
    "    image_input=image_input.to(\"cuda\")\n",
    "    logits = model(text_input, image_input)\n",
    "    preds=sigmoid(logits)\n",
    "\n",
    "    # Collect predictions and labels for metrics calculation\n",
    "    test_preds_classes = (preds > 0.5).int().cpu().numpy()\n",
    "    all_preds.extend(test_preds_classes)\n",
    "\n",
    "test_pred_labels=[num2label[el.item()] for el in all_preds]\n",
    "test_set[\"label\"]=test_pred_labels\n",
    "test_set_=test_set.drop(columns=[\"text\",\"image\"])\n",
    "write_json(summary_dir_path,test_set_,\"dev_preds.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W4Bl4nGqkh8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOdgnY1Iqkh8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYN3rz68qkh9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_praktikum_env",
   "language": "python",
   "name": "nlp_praktikum_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
